import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score
from astropy.io import fits
import os
from tqdm import tqdm
from multiprocessing import Pool


os.makedirs("results", exist_ok=True)

# =========================
# 1) Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª FITS Ùˆ Kepler CSV
# =========================
fits_file = "fits_results.csv"
kepler_file = "Kepler Data.csv"

# Ù‚Ø±Ø§Ø¡Ø© Ù†ØªØ§Ø¦Ø¬ FITS
df_results = pd.read_csv(fits_file)
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ FITS: {len(df_results)}")
print("ğŸ“‚ Ø£Ø¹Ù…Ø¯Ø© FITS Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆØ­ÙŠØ¯:", df_results.columns.tolist())

# ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
if "file" in df_results.columns:
    df_results.rename(columns={"file": "filename"}, inplace=True)
if "best_period_days" in df_results.columns:
    df_results.rename(columns={"best_period_days": "period"}, inplace=True)
if "kepid" not in df_results.columns:
    df_results["kepid"] = np.nan  # Ù„Ùˆ Ù…Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø£ØµÙ„Ø§Ù‹

# numeric conversion
df_results["kepid"] = pd.to_numeric(df_results["kepid"], errors="coerce")
df_results["period"] = pd.to_numeric(df_results["period"], errors="coerce")

print("ğŸ“‚ Ø£Ø¹Ù…Ø¯Ø© FITS Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯:", df_results.columns.tolist())

# Ù‚Ø±Ø§Ø¡Ø© Kepler CSV Ù…Ø¹ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙØ§ØµÙ„
try:
    kepler_df = pd.read_csv(kepler_file, sep=None, engine="python")
except Exception:
    # Ø¥Ø°Ø§ ÙØ´Ù„ autodetect separatorØŒ Ù†Ø¬Ø±Ø¨ semicolon
    kepler_df = pd.read_csv(kepler_file, sep=";", engine="python")

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø£Ùˆ Ø§Ù„ÙØ§Ø±ØºØ©
kepler_df = kepler_df.loc[:, ~kepler_df.columns.str.contains('^Unnamed')]
# ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
if "kepid" not in kepler_df.columns:
    if "koi_period" in kepler_df.columns:
        kepler_df.rename(columns={"koi_period": "period"}, inplace=True)

kepler_df["kepid"] = pd.to_numeric(kepler_df.get("kepid", np.nan), errors="coerce")
kepler_df["period"] = pd.to_numeric(kepler_df.get("period", np.nan), errors="coerce")

print("ğŸ“‚ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ Kepler Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:", kepler_df.columns.tolist())

# =========================
# 2) Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø±Ø¨Ø·
# =========================
merged = pd.DataFrame()
# Ø­Ø§ÙˆÙ„ Ø§Ù„Ø±Ø¨Ø· Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ù€ kepid
if "kepid" in df_results.columns and "kepid" in kepler_df.columns:
    merged = pd.merge(df_results, kepler_df, on="kepid", how="inner", suffixes=("_res", "_koi"))

# Ù„Ùˆ ÙØ§Ø±ØºØŒ Ø­Ø§ÙˆÙ„ Ø§Ù„Ø±Ø¨Ø· Ø¨Ù€ period Ø§Ù„Ø£Ù‚Ø±Ø¨
if merged.empty:
    print("âš ï¸ Ø§Ù„Ø±Ø¨Ø· Ø¨Ù€ kepid ÙØ´Ù„ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø±Ø¨Ø· Ø¨Ù€ period Ø§Ù„Ø£Ù‚Ø±Ø¨")
    matches = []
    for _, r in df_results.iterrows():
        if pd.isna(r["period"]):
            continue
        diffs = abs(kepler_df["period"] - r["period"])
        idx = diffs.idxmin()
        matched = kepler_df.loc[idx].copy()
        for col in df_results.columns:
            matched[col] = r.get(col)
        matched["period_diff"] = abs(matched["period"] - r["period"])
        matches.append(matched)
    merged = pd.DataFrame(matches)

print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(merged)} Ù†ØªØ§Ø¦Ø¬ ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ­Ù‚Ù‚.")

# =========================
# 3) Ø¥Ù†Ø´Ø§Ø¡ true_label Ùˆ predicted
# =========================
if "koi_disposition" in merged.columns:
    merged["true_label"] = merged["koi_disposition"].apply(lambda x: 1 if str(x).upper() == "CONFIRMED" else 0)
else:
    merged["true_label"] = 0

# Predicted by power
if "max_power" in merged.columns:
    threshold = merged["max_power"].median()
    merged["pred_by_power"] = merged["max_power"].apply(lambda x: 1 if pd.notna(x) and x > threshold else 0)
else:
    merged["pred_by_power"] = 0

# Predicted by period match
if "period_diff" in merged.columns:
    merged["pred_by_period"] = merged["period_diff"].apply(lambda x: 1 if pd.notna(x) and x <= 0.01 * merged["period"] else 0)
else:
    merged["pred_by_period"] = 0

# =========================
# 4) Metrics
# =========================
metrics = {}
metrics["by_power"] = {
    "accuracy": float(accuracy_score(merged["true_label"], merged["pred_by_power"])),
    "precision": float(precision_score(merged["true_label"], merged["pred_by_power"], zero_division=0)),
    "recall": float(recall_score(merged["true_label"], merged["pred_by_power"], zero_division=0))
}
metrics["by_period"] = {
    "accuracy": float(accuracy_score(merged["true_label"], merged["pred_by_period"])),
    "precision": float(precision_score(merged["true_label"], merged["pred_by_period"], zero_division=0)),
    "recall": float(recall_score(merged["true_label"], merged["pred_by_period"], zero_division=0))
}

print("ğŸ“Š Metrics:", metrics)

import time


os.makedirs("results", exist_ok=True)

# =========================
# Ù‚Ø±Ø§Ø¡Ø© Ù†ØªØ§Ø¦Ø¬ FITS ÙÙ‚Ø·
# =========================
fits_file = "fits_results.csv"
df_results = pd.read_csv(fits_file)
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ FITS: {len(df_results)}")
df_results.rename(columns={"file": "filename", "best_period_days": "period"}, inplace=True)
df_results["kepid"] = pd.to_numeric(df_results.get("kepid", np.nan), errors="coerce")
df_results["period"] = pd.to_numeric(df_results["period"], errors="coerce")
print("ğŸ“‚ Ø£Ø¹Ù…Ø¯Ø© FITS Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯:", df_results.columns.tolist())

# =========================
# Feature Extraction
# =========================
def extract_features(lightcurve_file):
    path = f"lightcurves/{lightcurve_file}"
    try:
        if lightcurve_file.endswith(".fits"):
            with fits.open(path) as hdul:
                data = hdul[1].data
                time_arr = data['TIME']
                flux = data['PDCSAP_FLUX'] if 'PDCSAP_FLUX' in data.columns.names else data['SAP_FLUX']
        else:
            lc = pd.read_csv(path)
            time_arr = lc['time'].values
            flux = lc['flux'].values

        mask = ~np.isnan(time_arr) & ~np.isnan(flux)
        time_arr, flux = time_arr[mask], flux[mask]

        if len(flux) == 0:
            return pd.Series({'depth': np.nan, 'duration': np.nan, 'snr': np.nan})

        median_flux = np.median(flux)
        min_flux = np.min(flux)
        depth = median_flux - min_flux

        below_median = time_arr[flux < median_flux]
        duration = (np.max(below_median) - np.min(below_median)) if below_median.size > 0 else 0
        snr = depth / np.std(flux)

        return pd.Series({'depth': depth, 'duration': duration, 'snr': snr})

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ù…Ø¹ {lightcurve_file}: {e}")
        return pd.Series({'depth': np.nan, 'duration': np.nan, 'snr': np.nan})

# =========================
# ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª
# =========================
start_time = time.time()
features = df_results['filename'].apply(extract_features)
df_results = pd.concat([df_results, features], axis=1)
end_time = time.time()

# =========================
# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
# =========================
df_results.to_csv("results/validation_with_features.csv", index=False)
print("âœ… ØªÙ… Ø­ÙØ¸ Validation + Features ÙÙŠ: results/validation_with_features.csv")
print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {end_time - start_time:.2f} Ø«Ø§Ù†ÙŠØ©")